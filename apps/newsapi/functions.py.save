import requests
import datetime
from .models import Newskey, Article

def perform_article_search(q, qInTitle, news_from, languages, domains, excludeDomains):
    # Url for API
    url = "http://newsapi.org/v2/everything"

    # Obtain key from database
    key = Newskey.objects.all()[0].key

    # Create query
    params = {"q": q, "qInTitle": qInTitle, "from": news_from, "sortBy": "publishedAt", "apiKey": key, "language": languages, "domains":domains, "excludeDomains": excludeDomains }

    response = requests.get(url, params = params)

    # Verify whether response status is between 200 and 400
    if not response:
        print("Error obtaining news:\n", response.json(), "\n\n")
        return None
    else:
        articles = response.json()["articles"]

    if len(articles) == 0:
        return None
    else:
        return articles

def get_articles(transaction):
    # Obtain news for the last week only
    date = datetime.date.today() - datetime.timedelta(days=3)
    news_from = date.strftime("%Y-%m-%d")

    # Langugages that we search for
    languages = ("en")

    # Domains to search for
    # domains = ("cnbc.com", "marketwatch.com", "bloomberg.com", "reuters.com", "investopedia.com", "investing.com", "wsj.com", "ft.com", "fool.com", "seekingalpha.com", "businessinsider.nl","businessinsider.com","news.yahoo.com","digitimes.com","wccftech.com","nasdaq.com","prnewswire.com","digitaltrends.com","rappler.com","electrek.co","energycentral.com")
    domains = ()
    # Domains to exclude
    excludeDomains = ("rlsbb.ru", "southernsavers.com", "dailymail.co.uk", "chowhound.com", "freerepublic.com", "soldiersystems.net", "openers.jp", "mmo-champion.com","dailymail.co","lewrockwell.com","journals.plos.org","globalresearch.ca","sports.yahoo.com","slickdeals.net","mspoweruser.com","abc.net.au","neowin.net","phoronix.com","tomshardware.com","hotukdeals.com","dealabs.com","drivers.softpedia.com","dealcatcher.com", "kqed.org")

    # Keywords to search for in text
    q = ()
    # Keyword to search for in title
    qInTitle = transaction.stock.ticker

    # Minimum amount of articles required
    n_articles_required = 3

    all_articles  = []
    articles = perform_article_search(q, qInTitle, news_from, languages, domains, excludeDomains) 
    if articles is not None:
        all_articles = all_articles + articles

    if len(all_articles) < n_articles_required:
        qInTitle = ()
        q = transaction.stock.ticker
        articles = perform_article_search(q, qInTitle, news_from, languages, domains, excludeDomains)
        if articles is not None:
            all_articles = all_articles + articles
        if len(all_articles) < n_articles_required:
            qInTitle = transaction.stock.name
            q = ()
            articles = perform_article_search(q, qInTitle, news_from, languages, domains, excludeDomains)
            if articles is not None:
                all_articles = all_articles + articles
            if len(all_articles) < n_articles_required:
                qInTitle = ()
                q = transaction.stock.name
                articles = perform_article_search(q, qInTitle, news_from, languages, domains, excludeDomains)
                if articles is not None:
                    all_articles = all_articles + articles

    # First delete all of the articles before retrieving new ones
    if len(all_articles) >= n_articles_required:
        delete_articles(transaction)

    for article in all_articles:
        # Verify whether article does not exist yet in transaction
        if len(Article.objects.filter(title=article["title"])) == 0:
            foo = Article(transaction=transaction, title=article["title"], url=article["url"], read=False)
            foo.save()

# This function deletes all of the articles for the given transaction
def delete_articles(transaction):
    articles = Article.objects.filter(transaction=transaction)
    articles.delete()
